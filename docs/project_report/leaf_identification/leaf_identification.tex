\chapter{Leaf Identification}

\section{Introduction}
In this chapter we discuss the mechanics of leaf classification exploring two different approaches and evaluating the success or otherwise of both.

For this stage of the project it is more productive to develop the vision software using Java SE on the desktop instead of in the Android environment. As the same OpenCV library is being used both on the desktop and on Android it should make porting the techniques to another platform straightforward. This should offer an easier development path both in debugging and deployment as we remove the need to manage the extra complexity involved in deploying to a different platform. 


\section{Algorithmic Approach}

This approach revolves around extracting certain features from an image, such as edges, colour, textures, contours, etc. and creating a model to match that leaf type.

It is important to keep the number of transformations performed on the individual image minimal as this project is targeting a resource starved environment. The Android application will also be required to display a number of frames per second to provide a sense of real-time feedback to the user, although this may be achieved by processing only every second or third frame.

In this section we will concentrate primarily on identifying shapes, which removes the necessity to store and manipulate large sized images and colour information. 

The concept being tested in this section is to perform a series of transformations to differentiate the leaf from the background, this is hoped to enhance the leaf’s edges for a given frame and finally using contours to extract the shape of that leaf. 

\subsection{Resource Utilisation}

Early in the process we can combine the BGR (Blue, Green, Red) colour channels into a single grayscale channel and we may also sub-sample the image. These two optimisations will help minimise both CPU and memory utilisation.

Just to briefly describe the the gain from converting and storing the image in grayscale, we will analyse how the image is stored in OpenCV. The IplImage type that OpenCV operates on stores an image as a number of bitmaps, one for each channel of colour information. For this project we define that each pixel will have a colour depth of 256 shades per channel and this is stored in a 8bit unsigned structure, giving 1 byte per colour channel per pixel  \cite{morganIPL06}.

For example the video stream been analysed in the test environment has an resolution of 640 x 480 pixels which is 307,200 individual pixels. For each pixel we must record colour values for red, green and blue which require 1 byte each. To calculate the necessary memory we multiply 307,200 pixels by the 3 bytes which gives us 900 kilobytes per frame. If we are to convert the image to a single grayscale channel we save 600 kilobytes per frame. This equates to a considerable memory saving, especially when we see as many as 24 frames per second being processed.

The actual image size returned depends on the model of the device, for example the device being used for this project is the Google Nexus One which is capable of capturing 720 x 480 pixels at 20 frames per second.

This analysis is slightly simplistic as the IplImage type contains padding and metadata, but gives a good overview of the savings involved.

At this stage we are unable to provide an analysis of memory savings from sub-sampling performed on the image, as the optimal size for shape analysis is yet to be discovered.


\subsection{Preparing the Image}

\begin{figure}[h!]
\centering
    \includegraphics[width=1.0\textwidth]{leaf_identification/images/stages_of_processing.png}
    \caption{Stages of Processing: Input raw image (top left), Grayscale image with histogram equalisation (top right), Binary image after adaptive threshold (bottom right), Dilated binary image (bottom left)}%
    \label{stages_of_processing}
\end{figure}

Once a frame is retrieved from the camera's sensor we must preprocess the image before attempting to classify its features. The goal of preparing the image is twofold, firstly we need to perform filtering on the image in order to separate the foreground objects from the background, and secondly the \emph{cvFindContours()} function requires a binary image as input.

In Figure \ref{stages_of_processing} we can see a visual representation of the transforms performed on the image. The order and configuration of these have been determined through experimentation until a reasonable result was achieved. Below we describe the transformations and the order in which they are utilised.


\subsubsection{Convert to grayscale}
As discussed earlier we attempt to optimise any processing by minimising both the memory and CPU utilisation. As colour information is unnecessary for this approach we remove it at the first available opportunity.

It would be also advisable at this stage to sub-sample the image to the smallest size possible while still achieving good matches.

\subsubsection{Histogram Equalisation}
The image quality from mobile phone sensors vary widely, but one common problem that would hinder the classification of shapes in an image is the lack of contrast. With histogram equalisation we redistribute brightness across the range of the image, expanding the dynamic range of the image which increases the contrast \cite{kuntz09}.


\subsubsection{Binary conversion}
To detect contours we must first convert an image into its binary representation. There are a number of ways to perform such a conversion, with threshold and edge detection being most common.

In Figure \ref{preprocessing} we see the results of processing an image with the Canny Edge detection (left) and the Adaptive Threshold (right) algorithms. On inspection of the output it is clear that the adaptive threshold function provides a better silhouette of the central object, edge detection gives a broken ‘contour’ of the leaf. 

To evaluate which technique offers the best result, we must understand what a contour is and how they are detected. We will discuss this further later in the document but for our purposes here a contour is a curve drawn on the boundary between light and dark areas of a binary image, these are typically closed and may contain further nested curves.

With this in mind we see in Figure \ref{preprocessing} that the adaptive threshold approach offers a clearer representation of the shape. At this point it might be important to point out that it may be useful to utilise edge detection to enhance the edges of an image before conversion to its binary form. This may help to add further definition to the foreground object.

When applying a threshold filter to an image we are essentially making a decision to accept or reject a particular pixel based on a certain threshold value \cite{bradski08}. OpenCV provides a number of options for how the threshold is to be calculated.

Adaptive threshold is a technique where the threshold value is variable, this threshold value is calculated as a weighted average from a predetermined pixel region around the current pixel.

Once this transform is applied to the input image we are returned a binary version of that image which can then be used to find contours, we will discuss this further in the next section.

\begin{figure}[h!]
\centering
    \includegraphics[width=1.0\textwidth]{leaf_identification/images/convert_to_binary.png}
    \caption{Binary image conversion: Canny Edge detection (left) and Adaptive Threshold (right)}%
    \label{preprocessing}
\end{figure}


\subsubsection{Dilate}
After an image has been converted to its binary representation, we must to attempt to eliminate noise which may have crept in from the previous processing steps. Noise concentrated on a boundary in an image may adversely affect the contours that are extracted. 

Due to the nature of the images we are processing, it is important to perform minimal noise reduction. If we were to over correct for noise we would inadvertently affect the shape of the detected object, decreasing the systems accuracy.

Typically erosion and dilation transforms are applied to an image in order to decrease or remove noise \cite{bradski08}. Dilation has the effect of making bright regions in an image glow or expand, while erosion has the opposite effect. These functions use a pixel mask with an anchor point to scan the input image and calculate the local maximum value in the case of dilate or local minimum value in the case of erode. As this mask is then passed over the image, the anchor point pixel value is set to that of the local maximum or minimum for this mask.

As stated previously we aim to preform only minimal noise correction, it was decided to apply a dilate transform with a small mask size. Dilation was decided upon as the type of images being processed my have very thin borders and this technique is the best for preserving these edges.




\subsubsection{Additional techniques}
We are battling a number of constraints in this environment namely lighting conditions, shade variation between background and foreground items, non-uniformity in the shape of the leaves as well as designing performant software. After experimenting with a number of different techniques for creating a good binary representation the procedure outlined above was settled on.

However, a number of enhancements could be made to the process with the aim of increasing accuracy. 

\begin{itemize}
\item Enhance the edges of the preprocessed image using edge detection
\item Alternate the type of preprocessing performed on each frame
\item Background subtraction
\item Dynamic preprocessing using use values from the last captured image such as average brightness and shade range between average brightness and darkness to process the image based on its actual conditions
\end{itemize}


\subsection{Identifying features}

As seen in Figure \ref{preprocessing}, we should now have a clean binary image with the object of interest being identifiable to the eye. We must now use some mechanism to detect shapes and compare them to a store of pre-captured shapes in an efficient manner. 


\begin{figure}[h!]
\centering
    \includegraphics[width=1\textwidth]{leaf_identification/images/contours.png}
    \caption{Contour detection: Early crude detection (left), Extracting an accurate reference contour for a leaf (right)}%
    \label{contours}
\end{figure}

\subsubsection{Contours}

OpenCV provides the function \emph{cvFindContours()} which maps boundaries in a binary image. A contour is essentially a list of points that represent a curve in an image \cite{bradski08}. While obvious, it is important to point out that the contours returned are the result of the image preprocessing and may not represent the actual items being captured.

The function returns a contour tree, which is a nested structure of all the contours discovered. This concept of contours containing further contours is useful for certain forms of shape analysis. A individual contour is returned as a sequence of \emph{(x,y)} points that describe the edge of an object. In Figure \ref{contours} the reference contour (left) is composed of a sequence of some 699 points, we consider this the raw contour.


\subsubsection{Polygon Approximations}
In the returned nested structure of raw contours we have a contour that matches the binary shape of interest. We need to generalise the shape of the contour to create a more generic representation of the shape. This will help to correct for defects in the image and the physical leaf, to remove artifacts from the processing and to minimise the number of points in the contour sequence for storage. 

We apply the \emph{cvApproxPoly()} polygon approximation function which attempts to approximate the contour with that of one containing fewer vertices's. OpenCV provides an implementation of the Ramer-Douglas-Peucker approximation algorithm which we use to perform this approximation. This function takes the contour and a precision value as an input. The precision value defines the minimum distance points must be apart before inclusion in the new contour.

The algorithm begins by joining the two points in the polygon with the greatest distance between them, then recursively joins all the points until the precision value is encountered \cite{ramer72, bradski08}.

Once we apply this approximation function to our raw contour in Figure \ref{contours_raw_approx} we reduce the number of points from 699 to 126 while retaining the shape of the object.

\begin{figure}[h!]
\centering
    \includegraphics[width=1\textwidth]{leaf_identification/images/contours_raw_approx.png}
    \caption{Polygon Approximation: Raw contour (left) containing 699 points, Contour approximation (right) containing 126 points}%
    \label{contours_raw_approx}
\end{figure}

\subsubsection{Shape Matching}
Now that we have accurate contours from the object being captured by the camera and a collection of reference contours from known leaf types we must compare these shapes to categorise the object.

Again OpenCV provides mechanisms to perform this comparison, they are contained in the function \emph{cvMatchShapes()}. This function takes two contours and a value to select the comparison metric as its input, and returns a numeric value which describes how similar the shapes are with 0 being a match. This comparison is made using Hu invariant moments, which we will describe below.

From exposing many different silhouettes of leaves it was determined that a value of less than 0.03 can be considered to be a good match. However it is likely that with a larger database of leaf types to compare a more precise match may be required.

An image moment defines a gross characteristic of a contour which is calculated from the integral of all the pixels in the contour. These moment’s can be used to compare two contours, but will only match contours of the same shape, size and rotation.

Hu invariant moments provide the ability to match shapes under translation, so changes in their scale, rotation or reflection will not effect a match being made \cite{shutler02, rizon06}.

\colorbox{red}{would be nice to expand on hu moments}

\subsubsection{Additional techniques}
Using Hu invariant moments to compare two shapes has proven to be quite accurate, but it is not the only approach that could have taken. A number of other options such as Freemans chain codes and calculating convexity defects from a convex hull offer interesting alternatives \cite{park05, iivarinen97}. 

As we can see in Figure \ref{chain_codes} that chain codes are conceptually simpler than Hu invariant moments. In part \emph{a} of the figure we can see a diagram with 8 marked arrows having either relative or absolute directions and each direction having a unique symbol. These symbols are used to record a change in direction in a shapes contour. We see in part \emph{b} a shape with a start point which is processed to create a representation for that shape seen in part \emph{c}. Finally in part \emph{d} we see a histogram for this recorded chain code. This histogram is used to identify that contour under a rotational transform. To compare the contour under transform we simply move the starting point in the histogram on one position wrapping the previous data point to the end of the histogram \cite{anderson06}.

\begin{figure}[h!]
\centering
    \includegraphics[width=.7\textwidth]{leaf_identification/images/chain_codes.png}
    \caption{Chain codes}%
    \label{chain_codes}
\end{figure}

Finally another way of identifying a shape involves calculating convexity defects from a convex hull, this is also a successful mechanism for identifying particular kinds of shapes. Generating a convex hull involves creating a shape with the least amount of vertices to contain the detected shape, in effect connecting the most protruding points in the shape. With this convex hull we then calculate the distance from the convex hull to the actual shape, which we call the convexity defects. Using this technique could potentially be useful to in creating a lookup mechanism.

\subsection{Storage and Retrieval}
As we have seen earlier we are able reduce our representation of a leaf shape from 699 to 126 points. This reduction in the number of points is important, but we still require a certain number of points to have sufficient definition in the reference leaf contour. Due to the variance in complexity of leaf shape, these reference leaf signatures will be of a variable length.

For our example leaf we will require in the region of 1008 bytes of memory to store this reference contour. This memory breaks down as follows. There are 126 points which are accessed through a linked list. Each point is stored in a data type called \emph{CvPoint} which contains two integers, and it can be assumed that each integer requires 4 bytes of memory. In addition to this we also need to store a string for the name of the leaf as well as potentially storing a URL to provide additional information about the tree.

Efficient lookup and storage of these leaf signatures is difficult. For this project the lookup was essentially a brute-force operation with each reference contour being compared to every contour of a particular complexity and size discovered in the image. Storage of the reference contours was preformed using a flat file containing each point. Storing this form of data in a relational structure would be massively inefficient.

It is clear that as the size of the reference library increases a lookup mechanism would be required, as it may be impossible to store all the reference contours in memory. It may be necessary to take some other properties of the contour into account for creating this lookup mechanism, these could include a contour complexity or length metric. 


\section{Machine Learning Approach}

The use of machine learning is a common solution to the computer vision problem. Machine learning enables software make a decision based on a collection of preprocessed data. This is done by creating a model from training data which we can compare against various inputs.

There are two major approaches we can take in order to identify an object.

\begin{description}
\item[Clustering] Uses unsupervised learning to group inputs based on features that are similar in some way. This can be useful for identifying previously unobserved clusters \cite{agarwal03}. 
\item[Classifier] A supervised learning technique for discovering which which of a predefined set of classes a new object belongs to.
\end{description}

To meet the goals of this project it would appear that a classifier would be the most suitable approach. Again OpenCV offers a number of supervised learning algorithms, each with their own strengths and weaknesses. The Haar classifier implementation that OpenCV provides offers good detection capabilities for rigid objects and characteristic views \cite{bradski08}.

This approach of building the classifier model on a server and deploying it to the handset satisfies the requirements of the project by enabling the leaf identification to be performed on the mobile device without network connectivity. 

However, using this approach removes the ability for the user to add newly discovered leaves to its database. This could be tackled by asking the user in the case of an unidentified leaf to annotate the leaf with its type (if known) and upload the labeled image to a server for later addition to the model.


\subsection{Training Set}

Initially generating training data for a classifier is a difficult and slow undertaking. To generate a reasonable classifier we need to provide between 5000 and 10,000 annotated images, less training data can be provided but the results will be less accurate \cite{bradski08, seo08}.

\begin{description}
\item[Positive images] This will consist of 90\% of the training data, images should contain different lighting, angles and backgrounds. Each image must be labeled and cropped to a predetermined size.
\item[Negative images] This set of images must not contain the object in question, 3000 or more images will be required.
\item[Test images] These will be the other 10\% of the annotated training data, these images will only be used to verify the accuracy of the classifier.
\end{description}

Obtaining these large quantities of images for varying leaf types is a difficult task. One interesting solution encountered is to record video of an object while moving the camera across different angles and changing the light source. We are then able to extract single frames from the video and use these images as input.

\subsection{Training Process}
A number of utilities are provided for training a Haar classifier, the key utilities are be outlined below. It may prove useful to automate this process as the training process may require several days to complete.

\subsubsection{Create Samples}
From our training set we must create training samples using the \emph{createsamples} utility which offers the following functionality \cite{sun08}.

\begin{itemize}
\item Create samples from one input image applying various distortions.
\item  Create samples from the library of training data without applying any transforms.
\item Create samples from ground truth from a single image with transformations.
\item Display the samples stored in the .vec training file format.
\end{itemize}

These samples must be of a uniform size, with 20x20 appearing to be quite commonly used. For each of the positive training sets we must generate a text file with one line per image detailing \langle filename\rangle \langle number\_of\_rectangles\rangle \langle x\_y\_coord\_of\_each\_rect\rangle$  
which we will use as input to the \emph{createsamples} utility \cite{adolf03}.

These sample images must then be packed into the \emph{.vec} training file, using the  \emph{createsamples} utility. In Figure \ref{creating_samples} the command to build the \emph{.vec} can be seen.

\begin{figure}[h!]
\begin{lstlisting}
opencv-createsamples -info positives/train.txt -vec data/positives.vec -num 1300 -w 20 –h 20 
\end{lstlisting}
\caption{Packing a .vec file with training samples}
\label{creating_samples}
\end{figure}

\subsubsection{Training}
The actual training of the model is performed by the utility \emph{haartraining}, which employs the Viola-Jones algorithm to create a cascade file. In Figure \ref{creating_detector} we see the sample parameters provided to the utility to begin training the model. Default values for hitrate, maxfalsealarm, weighttrimming and boosting type can be used in most cases leaving only a small number of parameters needing to be changed \cite{adolf03}.

\begin{figure}[h!]
\begin{lstlisting}
opencv-haartraining -data data/cascade -vec data/positives.vec -bg negatives/train.txt  -npos 1300 -nneg 5350 -nstages 30 -mem 1300 -mode ALL -w 20 -h 20 
\end{lstlisting}
\caption{Creating a detector}
\label{creating_detector}
\end{figure}

\subsubsection{Testing}
Once the training is complete, we may use the \emph{performance} utility to test the resulting cascade. In order to use the test images we mentioned earlier, we must generate a text file containing only the test image file names, one per line. Additionally we may specify \emph{-ni} flag in the \emph{performance} utility which will save a copy of each test image with a detected object having bounding box drawn around the object. In Figure \ref{testing_cascade} the parameters required to test the cascade are outlined.

\begin{figure}[h!]
\begin{lstlisting}
opencv-performance -data data/cascade -info positives/testing/testing.txt -w 20 -h 20 -rs 30
\end{lstlisting}
\caption{Testing the cascade}
\label{testing_cascade}
\end{figure}

\subsection{Outcome}
The use of machine learning in this type application offers a number of clear advantages such as enabling the classification to be performed locally on the handset, only requiring minimal local processing and better classification under varying conditions. But to get a working model running initially requires a large time investment.

Although it would have been interesting to develop this classifier, it was decided not to proceed. This decision was taken primarily due to the time required to generate a robust model for more than one type of leaf, the success of the algorithmic approach and also that is was not necessary to prove the concept of real time leaf detection on a mobile handset. For future work in this area a Haar classifier would be a definite avenue of approach.

